# @package _global_
defaults:
  - /pipeline: listops
  - /model: transformer

model:
  dropout: 0.
  n_layers: 6
  d_model: 256
  prenorm: false
  norm: batch

decoder:
  mode: pool

loader:
  batch_size: 10

optimizer:
  lr: 0.001
  weight_decay: 0.01

trainer:
  max_epochs: 50
  limit_train_batches: 1.0
  limit_val_batches: 1.0

callbacks:
  model_checkpoint:
    save_top_k: -1  
    save_on_train_epoch_end: true 
    every_n_epochs: 1             

train:
  seed: 1112